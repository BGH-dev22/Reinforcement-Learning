# Q-Learning GridWorld - Analyse des ParamÃ¨tres

## ğŸ¯ Objectif du projet

Ce projet a pour but de tester et analyser l'influence des principaux paramÃ¨tres du **Q-Learning** sur la performance d'un agent dans un environnement **GridWorld** simple.  

Les paramÃ¨tres Ã©tudiÃ©s sont :
- **Learning Rate (Î±)** : vitesse d'apprentissage
- **Discount Factor (Î³)** : importance des rÃ©compenses futures
- **Exploration Rate (Îµ)** : propension de l'agent Ã  explorer
- **Exploration Decay** : dÃ©croissance de l'exploration au fil du temps
- **Exploration Min** : exploration minimale autorisÃ©e

L'objectif est de visualiser comment ces paramÃ¨tres influencent la fonction de valeur, la politique optimale et la performance de l'agent.

---

## ğŸ“‚ Structure du projet

.
â”œâ”€â”€ agents/
â”‚ â”œâ”€â”€ q_learning.py # Classe QLearningAgent et fonctions d'entraÃ®nement
â”‚ â”œâ”€â”€ mc.py # (optionnel) Monte Carlo
â”‚ â”œâ”€â”€ value_iteration.py # ItÃ©ration de valeur
â”‚ â””â”€â”€ policy_iteration.py # ItÃ©ration de politique
â”œâ”€â”€ figures_qlearning/ # Graphiques gÃ©nÃ©rÃ©s pour analyse des paramÃ¨tres
â”œâ”€â”€ results/ # RÃ©sultats sauvegardÃ©s
â”œâ”€â”€ gridworld.py # DÃ©finition de l'environnement GridWorld
â”œâ”€â”€ test_agents.py # Script principal pour tester l'agent et analyser les paramÃ¨tres
â”œâ”€â”€ q_learning_agent.pkl # Q-table sauvegardÃ©e
â”œâ”€â”€ q_learning_test_results.png # Visualisation finale
â””â”€â”€ README.md # Ce fichier

yaml
Copier le code

---

## ğŸ“ Contenu du script `test_agents.py`

1. **Initialisation de l'environnement GridWorld** :
   - Taille de la grille : 5x5
   - Ã‰tat initial : `(0,0)`
   - Objectif : `(4,4)`
   - Obstacles : `(1,1)` et `(2,3)`

2. **CrÃ©ation de l'agent Q-Learning** :
   - ParamÃ¨tres configurables : `learning_rate`, `discount_factor`, `exploration_rate`, `exploration_decay`, `exploration_min`

3. **EntraÃ®nement de l'agent** :
   - Nombre d'Ã©pisodes : 100
   - Historique des rÃ©compenses et des pas effectuÃ©s

4. **Analyse et visualisation** :
   - Courbe d'apprentissage (rÃ©compenses cumulÃ©es)
   - Nombre de pas par Ã©pisode
   - Fonction de valeur V(s) = max_a Q(s,a)
   - Politique optimale Ï€(s) affichÃ©e avec des flÃ¨ches
   - Sauvegarde des visualisations (`q_learning_test_results.png`)

5. **Test de la politique apprise** :
   - Exploration dÃ©sactivÃ©e (`Îµ=0`)
   - VÃ©rification des performances de l'agent sur plusieurs Ã©pisodes

6. **Sauvegarde et chargement de la Q-table** :
   - Sauvegarde dans `q_learning_agent.pkl`
   - VÃ©rification que la Q-table chargÃ©e correspond Ã  l'originale

---

## ğŸ“Š Visualisation

Le script produit plusieurs visualisations permettant de comprendre l'impact des paramÃ¨tres sur :

- La vitesse et la stabilitÃ© de l'apprentissage
- La convergence vers la politique optimale
- La qualitÃ© des rÃ©compenses cumulÃ©es

---

## ğŸš€ ExÃ©cution

1. CrÃ©er un environnement virtuel et installer les dÃ©pendances :

```bash
python -m venv venv
source venv/bin/activate   # Linux/Mac
venv\Scripts\activate      # Windows
pip install numpy matplotlib
ExÃ©cuter le script principal :

bash
Copier le code
python test_agents.py
Les rÃ©sultats seront affichÃ©s et sauvegardÃ©s sous :

q_learning_test_results.png

q_learning_agent.pkl

ğŸ“Œ Notes importantes
L'agent utilise un epsilon-greedy policy, avec un exploration_decay pour rÃ©duire l'exploration progressivement.

Les paramÃ¨tres peuvent Ãªtre facilement modifiÃ©s dans test_agents.py pour analyser leur influence.

La limite maximale de pas par Ã©pisode est fixÃ©e Ã  50 pour Ã©viter que l'agent tourne indÃ©finiment dans la grille.

