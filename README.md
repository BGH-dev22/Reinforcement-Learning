# ğŸ§  TD(0) Agent avec Approximation LinÃ©aire â€” GridWorld

## ğŸ“˜ Description du projet
Ce projet implÃ©mente et visualise **un agent TD(0) (Temporal Difference)** avec **approximation linÃ©aire de la fonction de valeur** dans un environnement **GridWorld** personnalisÃ©.  
Lâ€™objectif est de dÃ©montrer comment un agent apprend Ã  estimer la valeur des Ã©tats et Ã  amÃ©liorer sa politique de dÃ©placement par renforcement progressif Ã  travers lâ€™exploration et la mise Ã  jour des poids linÃ©aires.

---

## ğŸ—ï¸ Structure du projet

ğŸ“‚ Project Root
â”‚
â”œâ”€â”€ agents/ # Contient les classes des agents (ex: LinearTDAgent)
â”‚ â”œâ”€â”€ init.py
â”‚ â””â”€â”€ agent_td.py
â”‚
â”œâ”€â”€ figures_td0/ # Dossier contenant les visualisations gÃ©nÃ©rÃ©es
â”‚ â”œâ”€â”€ learning_analysis.png
â”‚ â”œâ”€â”€ policy_visualization.png
â”‚ â””â”€â”€ weights_analysis.png
â”‚
â”œâ”€â”€ results_deep_qlearning/ # (optionnel) Dossier pour dâ€™autres expÃ©rimentations
â”‚
â”œâ”€â”€ gridworld.py # Environnement GridWorld personnalisÃ©
â”œâ”€â”€ test_agent_td.py # Script principal dâ€™entraÃ®nement et de visualisation TD(0)
â”œâ”€â”€ test_agents.py # (optionnel) Autres tests ou comparaisons dâ€™agents
â”‚
â”œâ”€â”€ td0_rewards.npy # RÃ©compenses enregistrÃ©es (format binaire)
â”œâ”€â”€ td0_rewards.txt # RÃ©compenses enregistrÃ©es (format texte)
â”œâ”€â”€ td0_weights.npy # Poids appris de lâ€™agent
â”‚
â”œâ”€â”€ venv/ # Environnement virtuel Python
â””â”€â”€ README.md # Ce fichier

yaml
Copier le code

---

## âš™ï¸ Installation

### 1ï¸âƒ£ CrÃ©er et activer un environnement virtuel
```bash
python -m venv venv
source venv/bin/activate       # sous Linux/Mac
venv\Scripts\activate          # sous Windows
2ï¸âƒ£ Installer les dÃ©pendances requises
bash
Copier le code
pip install numpy matplotlib seaborn
ğŸš€ ExÃ©cution
Pour lancer lâ€™expÃ©rience TD(0), exÃ©cute simplement :

bash
Copier le code
python test_agent_td.py
ğŸ”¹ Ce que le script fait :
Initialise un environnement MyGridWorld avec une taille de 6x6, un point de dÃ©part, un but et des obstacles.

CrÃ©e un agent TD(0) avec approximation linÃ©aire.

EntraÃ®ne lâ€™agent pendant 300 Ã©pisodes.

GÃ©nÃ¨re automatiquement plusieurs visualisations :

Courbe dâ€™apprentissage des rÃ©compenses

Ã‰volution du taux dâ€™exploration

Heatmap des poids appris

Politique optimale apprise

Trajectoire de lâ€™agent dans la grille

Sauvegarde les rÃ©sultats (rÃ©compenses, poids, figures).

ğŸ“Š Sorties gÃ©nÃ©rÃ©es
Fichier / Dossier	Description
figures_td0/learning_analysis.png	Courbes dâ€™Ã©volution des rÃ©compenses, pas et exploration
figures_td0/weights_analysis.png	Analyse dÃ©taillÃ©e des poids appris
figures_td0/policy_visualization.png	Politique finale et trajectoire optimale
td0_rewards.npy	Historique complet des rÃ©compenses (format NumPy)
td0_rewards.txt	RÃ©sumÃ© textuel avec statistiques
td0_weights.npy	Poids finaux de lâ€™agent (format NumPy)

ğŸ§© DÃ©tails de lâ€™implÃ©mentation
ğŸ”¸ Agent TD(0)
Lâ€™agent suit la mise Ã  jour suivante :

ğ›¿
=
ğ‘Ÿ
+
ğ›¾
ğ‘‰
(
ğ‘ 
â€²
)
âˆ’
ğ‘‰
(
ğ‘ 
)
Î´=r+Î³V(s 
â€²
 )âˆ’V(s)
ğ‘¤
â†
ğ‘¤
+
ğ›¼
â€‰
ğ›¿
â€‰
ğœ™
(
ğ‘ 
)
wâ†w+Î±Î´Ï•(s)
Fonction dâ€™approximation : linÃ©aire avec features
[x, y, xÂ², yÂ², xÂ·y, prox_goal, bias]

Politique : Îµ-greedy (avec dÃ©croissance exponentielle de Îµ)

Apprentissage : mise Ã  jour incrÃ©mentale Ã  chaque Ã©tape

ğŸ”¸ Environnement GridWorld
Taille : 6x6

DÃ©part : (0,0)

But : (5,5)

Obstacles : (1,1), (2,3), (3,3)

RÃ©compenses :

Positive Ã  lâ€™arrivÃ©e

NÃ©gative pour les obstacles ou les sorties de grille

ğŸ“ˆ Exemple de sortie console
yaml
Copier le code
================================================================================
ğŸ¯ DÃ‰MONSTRATION: Agent TD(0) avec Approximation LinÃ©aire
================================================================================

ğŸƒ EntraÃ®nement sur 300 Ã©pisodes...
  Episode 50/300 - Reward: -2.10, Steps: 40.2, Îµ: 0.61
  Episode 100/300 - Reward: -1.50, Steps: 28.4, Îµ: 0.37
  ...
âœ… EntraÃ®nement terminÃ©!
   Performance finale: 0.85
Les rÃ©sultats sont automatiquement enregistrÃ©s et affichÃ©s dans figures_td0/.

ğŸ“ Objectif pÃ©dagogique
Ce projet a pour but de :

Illustrer le fonctionnement de TD(0) dans un environnement discret.

Montrer comment une fonction de valeur peut Ãªtre apprise par approximation linÃ©aire.

Comprendre lâ€™impact du taux dâ€™apprentissage, du facteur de discount et de lâ€™exploration sur la convergence.

Fournir des visualisations claires pour interprÃ©ter les rÃ©sultats.
