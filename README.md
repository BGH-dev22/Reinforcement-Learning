1. ğŸ¯ Objectif du projet

Ce projet vise Ã  Ã©tudier lâ€™influence de la taille du GridWorld sur la performance dâ€™un agent Q-Learning.
Lâ€™environnement GridWorld est une grille carrÃ©e oÃ¹ lâ€™agent doit atteindre un objectif en Ã©vitant les obstacles.

Pour chaque taille de grille (de 3x3 Ã  10x10), lâ€™agent est entraÃ®nÃ© pendant plusieurs Ã©pisodes, et les retours cumulÃ©s (rÃ©compenses totales par Ã©pisode) sont mesurÃ©s afin dâ€™analyser comment la complexitÃ© de lâ€™environnement affecte lâ€™apprentissage.

2. ğŸ§© Structure du projet
Mini-RL-GridWorld/
â”‚
â”œâ”€ agents/
â”‚   â””â”€ q_learning.py          # DÃ©finition de lâ€™agent Q-Learning
â”‚
â”œâ”€ figures/                   # Images gÃ©nÃ©rÃ©es pour chaque taille de grille
â”‚   â”œâ”€ grid_3x3.png
â”‚   â”œâ”€ grid_4x4.png
â”‚   â””â”€ ...
â”‚
â”œâ”€ gridworld.py               # DÃ©finition de lâ€™environnement MyGridWorld
â”œâ”€ test_agents.py             # Script principal dâ€™analyse et de visualisation
â”œâ”€ cumulative_rewards.csv     # Fichier CSV contenant les retours cumulÃ©s
â”œâ”€ performance_gridworld.gif  # Animation de lâ€™Ã©volution des performances
â”œâ”€ analyse_complete.png       # Figure rÃ©capitulative complÃ¨te
â”œâ”€ reward_analyse/            # (Optionnel) Dossier dâ€™analyse complÃ©mentaire
â”œâ”€ venv/                      # Environnement virtuel Python
â””â”€ README.md                  # Ce fichier

3. âš™ï¸ Installation et setup
CrÃ©er un environnement virtuel
python -m venv venv

Activer lâ€™environnement

Windows (PowerShell) :

.\venv\Scripts\Activate.ps1


Windows (cmd) :

.\venv\Scripts\activate.bat


Linux / MacOS :

source venv/bin/activate

Installer les dÃ©pendances
pip install gymnasium numpy matplotlib imageio

4. ğŸš€ ExÃ©cution du projet

Assurez-vous que lâ€™environnement virtuel est activÃ©, puis exÃ©cutez :

python test_agents.py


Le script :

entraÃ®ne un agent Q-Learning sur plusieurs tailles de grilles (de 3x3 Ã  10x10),

enregistre les retours cumulÃ©s dans un fichier CSV (cumulative_rewards.csv),

gÃ©nÃ¨re un graphique par taille de grille dans le dossier figures/,

crÃ©e un GIF comparatif (performance_gridworld.gif),

produit une figure rÃ©capitulative complÃ¨te (analyse_complete.png) montrant :

lâ€™Ã©volution du retour cumulÃ© par Ã©pisode,

la performance moyenne selon la taille,

les courbes normalisÃ©es et les performances finales.

5. ğŸ“Š RÃ©sultats attendus

Plus la grille est grande, plus lâ€™agent met du temps Ã  apprendre une bonne politique.

Le retour moyen diminue gÃ©nÃ©ralement avec la taille, Ã  cause dâ€™un espace dâ€™Ã©tat plus complexe.

Le GIF permet de visualiser la progression de la performance pour chaque taille de grille.

6. ğŸ“ Fichiers gÃ©nÃ©rÃ©s automatiquement
Fichier / Dossier	Description
figures/	Graphiques de performance par taille
cumulative_rewards.csv	Retours cumulÃ©s pour chaque taille
performance_gridworld.gif	Animation de lâ€™Ã©volution des apprentissages
analyse_complete.png	Figure rÃ©capitulative finale
reward_analyse/	(Optionnel) Analyses complÃ©mentaires
7. ğŸ§  Ã€ propos du projet

Ce projet fait partie dâ€™une exploration des techniques dâ€™apprentissage par renforcement (RL) appliquÃ©es Ã  des environnements simples.
Il met en Ã©vidence comment la complexitÃ© de lâ€™environnement (taille de la grille) influence les performances dâ€™un agent Q-Learning.
