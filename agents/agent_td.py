import numpy as np
from typing import Tuple

class LinearTDAgent:
    """
    Agent utilisant l'approximation lin√©aire de la fonction action-valeur avec TD(0).
    Bas√© sur le pseudo-code de pr√©diction lin√©aire TD(0) on-policy.
    
    Q(s, a; w) ‚âà w·µÄ ¬∑ œÜ(s, a)
    
    Mise √† jour TD(0):
    Œ¥ = r + Œ≥ ¬∑ Q(s', a'; w) - Q(s, a; w)
    w ‚Üê w + Œ± ¬∑ Œ¥ ¬∑ œÜ(s, a)
    """
    
    def __init__(self, env, learning_rate=0.1, discount_factor=0.99, 
                 exploration_rate=1.0, exploration_decay=0.995, 
                 exploration_min=0.01):
        """
        Initialise l'agent TD(0) avec approximation lin√©aire.
        
        Args:
            env: L'environnement GridWorld
            learning_rate: Taux d'apprentissage (Œ±)
            discount_factor: Facteur de discount (Œ≥)
            exploration_rate: Taux d'exploration initial (Œµ)
            exploration_decay: D√©croissance de l'exploration
            exploration_min: Exploration minimale
        """
        self.env = env
        self.learning_rate = learning_rate  # alpha
        self.discount_factor = discount_factor  # gamma
        self.exploration_rate = exploration_rate
        self.exploration_decay = exploration_decay
        self.exploration_min = exploration_min
        
        # Nombre de features
        self.n_features = self._compute_n_features()
        self.n_actions = env.action_space.n
        
        # Vecteur de poids w (partag√© pour toutes les actions avec features d√©pendantes de l'action)
        # Dimension: n_features * n_actions (car œÜ(s,a) encode l'action)
        self.weights = np.zeros(self.n_features * self.n_actions)
        
        print(f"ü§ñ Agent TD(0) Lin√©aire initialis√©:")
        print(f"   - Features de base: {self.n_features}")
        print(f"   - Actions: {self.n_actions}")
        print(f"   - Dimension de w: {len(self.weights)}")
        print(f"   - Learning rate (Œ±): {learning_rate}")
        print(f"   - Discount (Œ≥): {discount_factor}")
    
    def _compute_n_features(self):
        """Calcule le nombre de features de base."""
        # Features: [x, y, x¬≤, y¬≤, x*y, distance_goal, bias]
        return 7
    
    def extract_features(self, state: Tuple[int, int], action: int) -> np.ndarray:
        """
        Extrait le vecteur de features œÜ(s, a) pour l'approximation lin√©aire.
        
        Le vecteur œÜ(s, a) encode √† la fois l'√©tat et l'action:
        - Pour chaque action, on a un bloc de features
        - Seul le bloc correspondant √† l'action est non-z√©ro
        
        Structure: œÜ(s, a) = [0...0, œÜ_base(s), 0...0]
                             ‚îî‚îÄaction 0‚îÄ‚îò‚îî‚îÄaction a‚îÄ‚îò‚îî‚îÄautres‚îÄ‚îò
        
        Features de base œÜ_base(s):
        1. Position x normalis√©e
        2. Position y normalis√©e  
        3. x¬≤ (non-lin√©arit√©)
        4. y¬≤ (non-lin√©arit√©)
        5. x*y (interaction)
        6. Proximit√© au goal (1 - distance normalis√©e)
        7. Bias (toujours 1)
        
        Args:
            state: Position (x, y)
            action: Action choisie
            
        Returns:
            Vecteur de features œÜ(s, a) de dimension (n_features * n_actions)
        """
        x, y = state
        size = self.env.size
        
        # Normalisation
        x_norm = x / (size - 1) if size > 1 else 0
        y_norm = y / (size - 1) if size > 1 else 0
        
        # Distance au goal (normalis√©e)
        goal = self.env.goals[0] if self.env.goals else (size-1, size-1)
        max_dist = 2 * (size - 1)
        dist_goal = (abs(x - goal[0]) + abs(y - goal[1])) / max_dist if max_dist > 0 else 0
        
        # Features de base pour cet √©tat
        base_features = np.array([
            x_norm,               # Position x
            y_norm,               # Position y
            x_norm ** 2,          # x¬≤
            y_norm ** 2,          # y¬≤
            x_norm * y_norm,      # Interaction x*y
            1 - dist_goal,        # Proximit√© au goal
            1.0                   # Bias
        ])
        
        # Cr√©er le vecteur complet œÜ(s, a)
        # Seul le bloc correspondant √† l'action est non-z√©ro
        phi = np.zeros(self.n_features * self.n_actions)
        start_idx = action * self.n_features
        end_idx = start_idx + self.n_features
        phi[start_idx:end_idx] = base_features
        
        return phi
    
    def get_q_value(self, state: Tuple[int, int], action: int) -> float:
        """
        Calcule Q(s, a; w) = w·µÄ ¬∑ œÜ(s, a)
        
        Args:
            state: √âtat (x, y)
            action: Action
            
        Returns:
            Valeur Q estim√©e
        """
        phi = self.extract_features(state, action)
        return np.dot(self.weights, phi)
    
    def get_all_q_values(self, state: Tuple[int, int]) -> np.ndarray:
        """Calcule Q(s, a; w) pour toutes les actions."""
        return np.array([self.get_q_value(state, a) for a in range(self.n_actions)])
    
    def choose_action(self, state: Tuple[int, int]) -> int:
        """
        Politique Œµ-greedy pour choisir l'action.
        
        Args:
            state: √âtat actuel
            
        Returns:
            Action choisie
        """
        if np.random.random() < self.exploration_rate:
            # Exploration
            return self.env.action_space.sample()
        else:
            # Exploitation: a = argmax_a Q(s, a; w)
            q_values = self.get_all_q_values(state)
            return np.argmax(q_values)
    
    def update(self, state: Tuple[int, int], action: int, 
               reward: float, next_state: Tuple[int, int], 
               done: bool = False):
        """
        Mise √† jour TD(0) selon le pseudo-code:
        
        œÜ_s = œÜ(s, a)
        œÜ_s2 = œÜ(s', a')  o√π a' est l'action choisie dans s'
        delta = r + (0 if done else Œ≥ ¬∑ w·µÄ¬∑œÜ_s2) - w·µÄ¬∑œÜ_s
        w ‚Üê w + Œ± ¬∑ delta ¬∑ œÜ_s
        
        Args:
            state: √âtat actuel s
            action: Action prise a
            reward: R√©compense re√ßue r
            next_state: √âtat suivant s'
            done: Episode termin√©?
        """
        # Extraire œÜ(s, a)
        phi_s = self.extract_features(state, action)
        
        # Calculer Q(s, a; w) = w·µÄ ¬∑ œÜ(s, a)
        q_current = np.dot(self.weights, phi_s)
        
        # Calculer la cible
        if done:
            # Si terminal, pas de valeur future
            target = reward
        else:
            # Choisir a' pour s' selon la politique (on-policy)
            next_action = self.choose_action(next_state)
            phi_s2 = self.extract_features(next_state, next_action)
            q_next = np.dot(self.weights, phi_s2)
            target = reward + self.discount_factor * q_next
        
        # Calcul de l'erreur TD
        delta = target - q_current
        
        # Mise √† jour des poids: w ‚Üê w + Œ± ¬∑ Œ¥ ¬∑ œÜ(s, a)
        self.weights += self.learning_rate * delta * phi_s
        
        # D√©croissance de l'exploration
        if done:
            self.exploration_rate = max(
                self.exploration_min,
                self.exploration_rate * self.exploration_decay
            )
    
    def get_policy(self, state: Tuple[int, int]) -> int:
        """Retourne l'action optimale selon la politique actuelle."""
        q_values = self.get_all_q_values(state)
        return np.argmax(q_values)
    
    def get_value(self, state: Tuple[int, int]) -> float:
        """Retourne V(s) = max_a Q(s, a; w)."""
        q_values = self.get_all_q_values(state)
        return np.max(q_values)
    
    def get_value_grid(self) -> np.ndarray:
        """
        G√©n√®re une grille compl√®te des valeurs d'√©tat.
        
        Returns:
            Matrice size x size avec V(s) pour chaque √©tat
        """
        size = self.env.size
        value_grid = np.zeros((size, size))
        
        for x in range(size):
            for y in range(size):
                value_grid[x, y] = self.get_value((x, y))
        
        return value_grid
    
    def reset_exploration(self, exploration_rate=1.0):
        """R√©initialise le taux d'exploration."""
        self.exploration_rate = exploration_rate
    
    def save_weights(self, filepath: str):
        """Sauvegarde les poids."""
        np.save(filepath, self.weights)
        print(f"‚úÖ Poids sauvegard√©s dans '{filepath}'")
    
    def load_weights(self, filepath: str):
        """Charge les poids."""
        self.weights = np.load(filepath)
        print(f"‚úÖ Poids charg√©s depuis '{filepath}'")


class LinearTDAgentWithEligibility(LinearTDAgent):
    """
    Agent TD avec approximation lin√©aire et eligibility traces TD(Œª).
    Extension du TD(0) pour utiliser les traces d'√©ligibilit√©.
    """
    
    def __init__(self, env, learning_rate=0.1, discount_factor=0.99, 
                 exploration_rate=1.0, exploration_decay=0.995, 
                 exploration_min=0.01, lambda_trace=0.8):
        """
        Args:
            lambda_trace: Param√®tre Œª pour les traces d'√©ligibilit√© (0 = TD(0), 1 = Monte Carlo)
        """
        super().__init__(env, learning_rate, discount_factor, 
                        exploration_rate, exploration_decay, exploration_min)
        
        self.lambda_trace = lambda_trace
        # Traces d'√©ligibilit√©: m√™me dimension que les poids
        self.eligibility_traces = np.zeros_like(self.weights)
        
        print(f"   - Lambda (Œª): {lambda_trace}")
    
    def reset_traces(self):
        """R√©initialise les traces d'√©ligibilit√© (d√©but d'√©pisode)."""
        self.eligibility_traces = np.zeros_like(self.weights)
    
    def update(self, state: Tuple[int, int], action: int, 
               reward: float, next_state: Tuple[int, int], 
               done: bool = False):
        """
        Mise √† jour TD(Œª) avec eligibility traces.
        
        œÜ_s = œÜ(s, a)
        Œ¥ = r + (0 if done else Œ≥ ¬∑ Q(s', a'; w)) - Q(s, a; w)
        e ‚Üê Œ≥¬∑Œª¬∑e + œÜ_s  (traces accumul√©es)
        w ‚Üê w + Œ± ¬∑ Œ¥ ¬∑ e
        
        Args:
            state: √âtat actuel s
            action: Action prise a  
            reward: R√©compense re√ßue r
            next_state: √âtat suivant s'
            done: Episode termin√©?
        """
        # Extraire œÜ(s, a)
        phi_s = self.extract_features(state, action)
        
        # Calculer Q(s, a; w)
        q_current = np.dot(self.weights, phi_s)
        
        # Calculer la cible
        if done:
            target = reward
        else:
            # Choisir a' pour s' selon la politique
            next_action = self.choose_action(next_state)
            q_next = self.get_q_value(next_state, next_action)
            target = reward + self.discount_factor * q_next
        
        # Erreur TD
        delta = target - q_current
        
        # Mise √† jour des traces d'√©ligibilit√©: e ‚Üê Œ≥¬∑Œª¬∑e + œÜ(s, a)
        self.eligibility_traces = (
            self.discount_factor * self.lambda_trace * self.eligibility_traces + phi_s
        )
        
        # Mise √† jour des poids avec les traces: w ‚Üê w + Œ± ¬∑ Œ¥ ¬∑ e
        self.weights += self.learning_rate * delta * self.eligibility_traces
        
        # D√©croissance exploration
        if done:
            self.exploration_rate = max(
                self.exploration_min,
                self.exploration_rate * self.exploration_decay
            )
            # Reset traces en fin d'√©pisode
            self.reset_traces()


def td0_linear_prediction(env, episodes=1000, alpha=0.1, gamma=0.99):
    """
    Fonction impl√©mentant exactement le pseudo-code TD(0) pour la pr√©diction lin√©aire.
    Version simplifi√©e pour d√©monstration p√©dagogique.
    
    Args:
        env: Environnement
        episodes: Nombre d'√©pisodes
        alpha: Learning rate
        gamma: Discount factor
        
    Returns:
        v: Vecteur de poids final
    """
    # D√©terminer la dimension d
    d = 7 * env.action_space.n  # Features par action
    
    # Initialiser v = 0
    v = np.zeros(d)
    
    print(f"üìö TD(0) Linear Prediction - Version Pseudo-code")
    print(f"   Episodes: {episodes}, Œ±={alpha}, Œ≥={gamma}, d={d}")
    
    for episode in range(episodes):
        # s, done = env.reset(), False
        s, _ = env.reset()
        done = False
        
        while not done:
            # Choisir action selon politique (ici Œµ-greedy simplifi√©e)
            if np.random.random() < 0.1:
                a = env.action_space.sample()
            else:
                # Calculer Q pour chaque action
                q_vals = []
                for act in range(env.action_space.n):
                    phi = extract_phi_simple(s, act, env)
                    q_vals.append(np.dot(v, phi))
                a = np.argmax(q_vals)
            
            # s2, r, done = env.step(a)
            s2, r, terminated, truncated, _ = env.step(a)
            done = terminated or truncated
            
            # phi_s = phi(s, a)
            phi_s = extract_phi_simple(tuple(s), a, env)
            
            # phi_s2 = phi(s2, a') o√π a' selon politique
            if not done:
                q_vals_next = []
                for act in range(env.action_space.n):
                    phi_next = extract_phi_simple(tuple(s2), act, env)
                    q_vals_next.append(np.dot(v, phi_next))
                a_prime = np.argmax(q_vals_next)
                phi_s2 = extract_phi_simple(tuple(s2), a_prime, env)
            else:
                phi_s2 = np.zeros_like(phi_s)
            
            # delta = r + (0 if done else gamma * np.dot(v, phi_s2)) - np.dot(v, phi_s)
            delta = r + (0 if done else gamma * np.dot(v, phi_s2)) - np.dot(v, phi_s)
            
            # v += alpha * delta * phi_s
            v += alpha * delta * phi_s
            
            # s = s2
            s = s2
        
        if (episode + 1) % 200 == 0:
            print(f"   Episode {episode + 1}/{episodes} termin√©")
    
    return v


def extract_phi_simple(state, action, env):
    """Helper pour extraire œÜ(s, a) dans la version pseudo-code."""
    x, y = state
    size = env.size
    n_features = 7
    n_actions = env.action_space.n
    
    x_norm = x / (size - 1) if size > 1 else 0
    y_norm = y / (size - 1) if size > 1 else 0
    
    goal = env.goals[0] if env.goals else (size-1, size-1)
    max_dist = 2 * (size - 1)
    dist_goal = (abs(x - goal[0]) + abs(y - goal[1])) / max_dist if max_dist > 0 else 0
    
    base_features = np.array([
        x_norm, y_norm, x_norm**2, y_norm**2, 
        x_norm * y_norm, 1 - dist_goal, 1.0
    ])
    
    phi = np.zeros(n_features * n_actions)
    start_idx = action * n_features
    phi[start_idx:start_idx + n_features] = base_features
    
    return phi