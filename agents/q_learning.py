import numpy as np
from typing import Tuple, Dict
from collections import defaultdict

class QLearningAgent:
    """
    Agent Q-Learning utilisant une Q-table pour l'apprentissage.
    
    Algorithme Q-Learning (off-policy TD control):
    Q(s, a) ‚Üê Q(s, a) + Œ±[r + Œ≥¬∑max_a' Q(s', a') - Q(s, a)]
    """
    
    def __init__(self, env, learning_rate=0.1, discount_factor=0.99,
                 exploration_rate=1.0, exploration_decay=0.995, 
                 exploration_min=0.01):
        """
        Initialise l'agent Q-Learning.
        
        Args:
            env: Environnement GridWorld
            learning_rate: Taux d'apprentissage Œ± (0 < Œ± ‚â§ 1)
            discount_factor: Facteur de discount Œ≥ (0 ‚â§ Œ≥ ‚â§ 1)
            exploration_rate: Taux d'exploration initial Œµ (0 ‚â§ Œµ ‚â§ 1)
            exploration_decay: Facteur de d√©croissance de Œµ (0 < decay ‚â§ 1)
            exploration_min: Valeur minimale de Œµ
        """
        self.env = env
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.exploration_rate = exploration_rate
        self.exploration_decay = exploration_decay
        self.exploration_min = exploration_min
        
        # Q-table: dictionnaire {state: {action: q_value}}
        self.q_table = defaultdict(lambda: defaultdict(float))
        
        # Nombre d'actions disponibles
        self.n_actions = env.action_space.n
        
        # Statistiques
        self.update_count = 0
        
        print(f"ü§ñ Agent Q-Learning initialis√©:")
        print(f"   - Learning rate (Œ±): {learning_rate}")
        print(f"   - Discount factor (Œ≥): {discount_factor}")
        print(f"   - Exploration rate (Œµ): {exploration_rate}")
        print(f"   - Exploration decay: {exploration_decay}")
        print(f"   - Exploration min: {exploration_min}")
        print(f"   - Nombre d'actions: {self.n_actions}")
    
    def get_q_value(self, state: Tuple[int, int], action: int) -> float:
        """
        Retourne la Q-value pour une paire (√©tat, action).
        
        Args:
            state: √âtat sous forme de tuple (x, y)
            action: Action (0=haut, 1=bas, 2=gauche, 3=droite)
            
        Returns:
            Q-value pour Q(state, action)
        """
        return self.q_table[state][action]
    
    def get_all_q_values(self, state: Tuple[int, int]) -> np.ndarray:
        """
        Retourne toutes les Q-values pour un √©tat donn√©.
        
        Args:
            state: √âtat sous forme de tuple (x, y)
            
        Returns:
            Array de Q-values [Q(s,0), Q(s,1), Q(s,2), Q(s,3)]
        """
        return np.array([self.get_q_value(state, a) for a in range(self.n_actions)])
    
    def get_max_q_value(self, state: Tuple[int, int]) -> float:
        """
        Retourne la valeur maximale Q pour un √©tat: max_a Q(s, a).
        
        Args:
            state: √âtat sous forme de tuple (x, y)
            
        Returns:
            max_a Q(state, a)
        """
        q_values = self.get_all_q_values(state)
        return np.max(q_values)
    
    def get_best_action(self, state: Tuple[int, int]) -> int:
        """
        Retourne la meilleure action pour un √©tat: argmax_a Q(s, a).
        
        En cas d'√©galit√©, choisit al√©atoirement parmi les meilleures actions.
        
        Args:
            state: √âtat sous forme de tuple (x, y)
            
        Returns:
            Meilleure action
        """
        q_values = self.get_all_q_values(state)
        max_q = np.max(q_values)
        
        # Trouver toutes les actions qui ont la valeur maximale
        best_actions = np.where(q_values == max_q)[0]
        
        # Choisir al√©atoirement parmi les meilleures en cas d'√©galit√©
        return np.random.choice(best_actions)
    
    def choose_action(self, state: Tuple[int, int]) -> int:
        """
        Choisit une action selon la strat√©gie Œµ-greedy.
        
        Avec probabilit√© Œµ: exploration (action al√©atoire)
        Avec probabilit√© 1-Œµ: exploitation (meilleure action)
        
        Args:
            state: √âtat actuel sous forme de tuple (x, y)
            
        Returns:
            Action choisie
        """
        if np.random.random() < self.exploration_rate:
            # Exploration: action al√©atoire
            return self.env.action_space.sample()
        else:
            # Exploitation: meilleure action selon Q-table
            return self.get_best_action(state)
    
    def update(self, state: Tuple[int, int], action: int, 
               reward: float, next_state: Tuple[int, int], 
               done: bool = False):
        """
        Met √† jour la Q-table avec la r√®gle de Q-Learning.
        
        Q(s, a) ‚Üê Q(s, a) + Œ±[r + Œ≥¬∑max_a' Q(s', a') - Q(s, a)]
        
        Si l'√©pisode est termin√© (done=True):
        Q(s, a) ‚Üê Q(s, a) + Œ±[r - Q(s, a)]
        
        Args:
            state: √âtat actuel (x, y)
            action: Action prise
            reward: R√©compense re√ßue
            next_state: √âtat suivant (x', y')
            done: True si l'√©pisode est termin√©
        """
        # Q-value actuelle
        current_q = self.get_q_value(state, action)
        
        # Calcul de la cible (target)
        if done:
            # √âtat terminal: pas de valeur future
            target = reward
        else:
            # Q-Learning: utilise max_a' Q(s', a') (off-policy)
            max_next_q = self.get_max_q_value(next_state)
            target = reward + self.discount_factor * max_next_q
        
        # Erreur TD (TD-error)
        td_error = target - current_q
        
        # Mise √† jour de la Q-value
        new_q = current_q + self.learning_rate * td_error
        self.q_table[state][action] = new_q
        
        # Statistiques
        self.update_count += 1
        
        # D√©croissance de l'exploration (uniquement en fin d'√©pisode)
        if done:
            self.exploration_rate = max(
                self.exploration_min,
                self.exploration_rate * self.exploration_decay
            )
    
    def get_policy(self, state: Tuple[int, int]) -> int:
        """
        Retourne l'action de la politique gloutonne (greedy).
        √âquivalent √† get_best_action, mais nom plus explicite.
        
        Args:
            state: √âtat (x, y)
            
        Returns:
            Action optimale selon la politique actuelle
        """
        return self.get_best_action(state)
    
    def get_value(self, state: Tuple[int, int]) -> float:
        """
        Retourne la valeur d'√©tat V(s) = max_a Q(s, a).
        
        Args:
            state: √âtat (x, y)
            
        Returns:
            Valeur d'√©tat V(s)
        """
        return self.get_max_q_value(state)
    
    def get_value_grid(self) -> np.ndarray:
        """
        G√©n√®re une grille compl√®te des valeurs d'√©tat.
        
        Returns:
            Matrice size x size avec V(s) pour chaque √©tat
        """
        size = self.env.size
        value_grid = np.zeros((size, size))
        
        for x in range(size):
            for y in range(size):
                value_grid[x, y] = self.get_value((x, y))
        
        return value_grid
    
    def get_policy_grid(self) -> np.ndarray:
        """
        G√©n√®re une grille compl√®te de la politique.
        
        Returns:
            Matrice size x size avec l'action optimale pour chaque √©tat
        """
        size = self.env.size
        policy_grid = np.zeros((size, size), dtype=int)
        
        for x in range(size):
            for y in range(size):
                policy_grid[x, y] = self.get_policy((x, y))
        
        return policy_grid
    
    def reset_exploration(self, exploration_rate: float = 1.0):
        """
        R√©initialise le taux d'exploration.
        
        Args:
            exploration_rate: Nouvelle valeur de Œµ
        """
        self.exploration_rate = exploration_rate
        print(f"‚úÖ Exploration rate r√©initialis√© √† {exploration_rate}")
    
    def get_state_action_counts(self) -> Dict:
        """
        Retourne le nombre d'√©tats et d'actions explor√©s.
        
        Returns:
            Dictionnaire avec les statistiques
        """
        n_states = len(self.q_table)
        n_state_action_pairs = sum(len(actions) for actions in self.q_table.values())
        
        return {
            'n_states_visited': n_states,
            'n_state_action_pairs': n_state_action_pairs,
            'n_updates': self.update_count
        }
    
    def print_statistics(self):
        """Affiche les statistiques de l'agent."""
        stats = self.get_state_action_counts()
        print("\nüìä Statistiques de l'agent Q-Learning:")
        print(f"   - √âtats visit√©s: {stats['n_states_visited']}")
        print(f"   - Paires (√©tat, action): {stats['n_state_action_pairs']}")
        print(f"   - Nombre de mises √† jour: {stats['n_updates']}")
        print(f"   - Taux d'exploration actuel: {self.exploration_rate:.4f}")
    
    def save_q_table(self, filepath: str):
        """
        Sauvegarde la Q-table dans un fichier.
        
        Args:
            filepath: Chemin du fichier de sauvegarde
        """
        import pickle
        
        # Convertir defaultdict en dict normal pour la sauvegarde
        q_table_dict = {state: dict(actions) for state, actions in self.q_table.items()}
        
        data = {
            'q_table': q_table_dict,
            'learning_rate': self.learning_rate,
            'discount_factor': self.discount_factor,
            'exploration_rate': self.exploration_rate,
            'exploration_decay': self.exploration_decay,
            'exploration_min': self.exploration_min,
            'update_count': self.update_count
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(data, f)
        
        print(f"‚úÖ Q-table sauvegard√©e dans '{filepath}'")
    
    def load_q_table(self, filepath: str):
        """
        Charge la Q-table depuis un fichier.
        
        Args:
            filepath: Chemin du fichier √† charger
        """
        import pickle
        
        with open(filepath, 'rb') as f:
            data = pickle.load(f)
        
        # Restaurer la Q-table
        self.q_table = defaultdict(lambda: defaultdict(float))
        for state, actions in data['q_table'].items():
            for action, q_value in actions.items():
                self.q_table[state][action] = q_value
        
        # Restaurer les param√®tres
        self.learning_rate = data['learning_rate']
        self.discount_factor = data['discount_factor']
        self.exploration_rate = data['exploration_rate']
        self.exploration_decay = data['exploration_decay']
        self.exploration_min = data['exploration_min']
        self.update_count = data['update_count']
        
        print(f"‚úÖ Q-table charg√©e depuis '{filepath}'")
        self.print_statistics()
    
    def get_q_table_as_array(self) -> np.ndarray:
        """
        Convertit la Q-table en array numpy pour visualisation.
        
        Returns:
            Array 3D de shape (size, size, n_actions)
        """
        size = self.env.size
        q_array = np.zeros((size, size, self.n_actions))
        
        for x in range(size):
            for y in range(size):
                state = (x, y)
                for a in range(self.n_actions):
                    q_array[x, y, a] = self.get_q_value(state, a)
        
        return q_array
    
    def __repr__(self) -> str:
        """Repr√©sentation textuelle de l'agent."""
        return (f"QLearningAgent(Œ±={self.learning_rate}, Œ≥={self.discount_factor}, "
                f"Œµ={self.exploration_rate:.4f}, states={len(self.q_table)})")


# Fonction utilitaire pour entra√Æner l'agent
def train_q_learning_agent(env, agent, episodes=100, verbose=True):
    """
    Entra√Æne un agent Q-Learning sur un environnement.
    
    Args:
        env: Environnement GridWorld
        agent: Agent QLearningAgent
        episodes: Nombre d'√©pisodes d'entra√Ænement
        verbose: Afficher les progr√®s
        
    Returns:
        Tuple (rewards_history, steps_history)
    """
    rewards_history = []
    steps_history = []
    
    for ep in range(episodes):
        state, _ = env.reset()
        done = False
        total_reward = 0
        steps = 0
        
        while not done:
            action = agent.choose_action(tuple(state))
            next_state, reward, terminated, truncated, _ = env.step(action)
            agent.update(tuple(state), action, reward, tuple(next_state), 
                        done=(terminated or truncated))
            
            state = next_state
            total_reward += reward
            steps += 1
            done = terminated or truncated
        
        rewards_history.append(total_reward)
        steps_history.append(steps)
        
        if verbose and (ep + 1) % 50 == 0:
            avg_reward = np.mean(rewards_history[-10:])
            avg_steps = np.mean(steps_history[-10:])
            print(f"Episode {ep+1}/{episodes} - Reward: {avg_reward:.2f}, "
                  f"Steps: {avg_steps:.1f}, Œµ: {agent.exploration_rate:.4f}")
    
    return rewards_history, steps_history